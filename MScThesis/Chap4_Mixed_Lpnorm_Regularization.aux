\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Mixed $L_p$-norm Regularization}{49}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Chap4_Mixed_Lpnorm_Regularization}{{4}{49}{Mixed $L_p$-norm Regularization}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Least-norm and regularized least-squares}{49}{section.4.1}}
\newlabel{LN_and_RLS}{{4.1}{49}{Least-norm and regularized least-squares}{section.4.1}{}}
\newlabel{eq:Fmd}{{4.1}{49}{Least-norm and regularized least-squares}{equation.4.1.1}{}}
\newlabel{eq:Inverse}{{4.2}{49}{Least-norm and regularized least-squares}{equation.4.1.2}{}}
\citation{TikhonovArsenin77}
\newlabel{eq:LeastNorm}{{4.3}{50}{Least-norm and regularized least-squares}{equation.4.1.3}{}}
\newlabel{eq:obj_func}{{4.5}{50}{Least-norm and regularized least-squares}{equation.4.1.5}{}}
\@writefile{brf}{\backcite{TikhonovArsenin77}{{50}{4.1}{equation.4.1.5}}}
\newlabel{eq:dphi_dm}{{4.6}{50}{Least-norm and regularized least-squares}{equation.4.1.6}{}}
\citation{LiOldenburg1996}
\citation{FarquharsonOldenburg98,SunLi14,Daubechies10}
\citation{LastKubik83}
\citation{Portniaguine1999}
\newlabel{Line_problem}{{4.7}{51}{Least-norm and regularized least-squares}{equation.4.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparative contour maps for various objective functions over a range of model values $[ m_1 ;\tmspace  +\thickmuskip {.2777em}m_2 ]$. (a) The mininum of the misfit function $\phi _d$ forms a line spanned by $\mathcal  {N} (\mathbf  {F})$ (red dash). The $least-norm$ solution is marked as a solid dot. (Middle) Regularization functions and gradient directions (blue arrows) for approximated $l_p$-norm measures of the model for (b) $p=2$, (d) $p=1$ and (f) $p=0$. The gradient directions are shown for two different starting models (black arrows). (bottom) Contour maps of the initial objective functions $\phi (m) = \phi _d + \phi _m$ for the same two starting models: $\mathbf  {m}^{(0)}_1$ (solid) and $\mathbf  {m}^{(0)}_2$ (dash). (c) In the $l_2$-norm case , the function has a global minimum regardless of the starting model, while for non-linear functions for (e) $p=1$ and (g) $p=0$, the objective function changes with respect to the starting model.\relax }}{52}{figure.caption.29}}
\newlabel{fig:IRLS_toy}{{4.1}{52}{Comparative contour maps for various objective functions over a range of model values $[ m_1 ;\;m_2 ]$. (a) The mininum of the misfit function $\phi _d$ forms a line spanned by $\mathcal {N} (\mathbf {F})$ (red dash). The $least-norm$ solution is marked as a solid dot. (Middle) Regularization functions and gradient directions (blue arrows) for approximated $l_p$-norm measures of the model for (b) $p=2$, (d) $p=1$ and (f) $p=0$. The gradient directions are shown for two different starting models (black arrows). (bottom) Contour maps of the initial objective functions $\phi (m) = \phi _d + \phi _m$ for the same two starting models: $\mathbf {m}^{(0)}_1$ (solid) and $\mathbf {m}^{(0)}_2$ (dash). (c) In the $l_2$-norm case , the function has a global minimum regardless of the starting model, while for non-linear functions for (e) $p=1$ and (g) $p=0$, the objective function changes with respect to the starting model.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Contour maps for various objective functions after convergence of the IRLS algorithm. (a) Final model obtained with the $l_2$-norm penalty on the model for two starting models at $\mathbf  {m}_1^{(0)}=[0.2;0.4]$ and $\mathbf  {m}_2^{(0)}=[0.6;0.2]$ for a fixed trade-off parameter ($\beta = 1e-4$). In both cases, the solution converges to the global minimum, which is also the $least-norm$ solution at $\mathbf  {m_{ln}}=[0.2;0.4]$. (b) Solution with the $l_1$-norm penalty for the same starting models and trade-off parameter, converging to a global minimum at $\mathbf  {m}^*=[0;0.5]$. This solution is sparse and can reproduce the data. (c) The same experiment is repeated for the $l_0$-norm penalty, converging to two different solutions depending on the relative magnitude of the starting model parameters. Both solutions are sparse and honor the data. \relax }}{53}{figure.caption.30}}
\newlabel{fig:IRLS_toy_result}{{4.2}{53}{Contour maps for various objective functions after convergence of the IRLS algorithm. (a) Final model obtained with the $l_2$-norm penalty on the model for two starting models at $\mathbf {m}_1^{(0)}=[0.2;0.4]$ and $\mathbf {m}_2^{(0)}=[0.6;0.2]$ for a fixed trade-off parameter ($\beta = 1e-4$). In both cases, the solution converges to the global minimum, which is also the $least-norm$ solution at $\mathbf {m_{ln}}=[0.2;0.4]$. (b) Solution with the $l_1$-norm penalty for the same starting models and trade-off parameter, converging to a global minimum at $\mathbf {m}^*=[0;0.5]$. This solution is sparse and can reproduce the data. (c) The same experiment is repeated for the $l_0$-norm penalty, converging to two different solutions depending on the relative magnitude of the starting model parameters. Both solutions are sparse and honor the data. \relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}$Lp$-norm and iterative re-weighted least squares}{53}{section.4.2}}
\@writefile{brf}{\backcite{LiOldenburg1996}{{53}{4.2}{section.4.2}}}
\@writefile{brf}{\backcite{Daubechies10}{{53}{4.2}{section.4.2}}}
\@writefile{brf}{\backcite{FarquharsonOldenburg98}{{53}{4.2}{section.4.2}}}
\@writefile{brf}{\backcite{SunLi14}{{53}{4.2}{section.4.2}}}
\citation{Ekblom73}
\citation{Lawson61}
\@writefile{brf}{\backcite{LastKubik83}{{54}{4.2}{section.4.2}}}
\@writefile{brf}{\backcite{Portniaguine1999}{{54}{4.2}{section.4.2}}}
\newlabel{eq:lpnorm}{{4.10}{54}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.10}{}}
\newlabel{eq:lpnorm}{{4.11}{54}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.11}{}}
\@writefile{brf}{\backcite{Ekblom73}{{54}{4.2}{equation.4.2.11}}}
\newlabel{eq:Ekblom}{{4.12}{54}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.12}{}}
\newlabel{eq:dphidm}{{4.13}{54}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.13}{}}
\@writefile{brf}{\backcite{Lawson61}{{54}{4.2}{equation.4.2.13}}}
\newlabel{eq:IRLS_phi}{{4.14}{54}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.14}{}}
\newlabel{eq:IRLS_dphidm}{{4.15}{55}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.15}{}}
\newlabel{eq:R_w}{{4.16}{55}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.16}{}}
\newlabel{eq:Lp_phi}{{4.17}{55}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.17}{}}
\newlabel{eq:dphi_dm_lp}{{4.19}{55}{$Lp$-norm and iterative re-weighted least squares}{equation.4.2.19}{}}
\citation{LastKubik83}
\citation{BarbosaSilva94,Ajo-Franklin07,Stocco09}
\citation{Chartrand07}
\@writefile{brf}{\backcite{LastKubik83}{{56}{4.2}{figure.caption.31}}}
\@writefile{brf}{\backcite{Ajo-Franklin07}{{56}{4.2}{figure.caption.31}}}
\@writefile{brf}{\backcite{BarbosaSilva94}{{56}{4.2}{figure.caption.31}}}
\@writefile{brf}{\backcite{Stocco09}{{56}{4.2}{figure.caption.31}}}
\@writefile{brf}{\backcite{Chartrand07}{{56}{4.2}{figure.caption.31}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces (a) Penalty function $\rho (x)$ for different approximate $l_p$-norm measures, and enlarged view near the region of influence of $\epsilon $, making the $l_p$-norm continuous at the origin. (b) IRLS weights $\mathbf  {r}(x)$ as a function of model function $\mathbf  {x}(m)$ and $p$-values and for a fix threshold parameter ($\epsilon =1e-2$). (c) Gradients $\mathbf  {g}(x)$ of the model penalty function for various $p$-values. Note that the gradients are on a logarithmic scale due to the rapid increase as ${x}_i \rightarrow \epsilon $ for $p < 1$.\relax }}{57}{figure.caption.31}}
\newlabel{fig:Lp_r_dphidm}{{4.3}{57}{(a) Penalty function $\rho (x)$ for different approximate $l_p$-norm measures, and enlarged view near the region of influence of $\epsilon $, making the $l_p$-norm continuous at the origin. (b) IRLS weights $\mathbf {r}(x)$ as a function of model function $\mathbf {x}(m)$ and $p$-values and for a fix threshold parameter ($\epsilon =1e-2$). (c) Gradients $\mathbf {g}(x)$ of the model penalty function for various $p$-values. Note that the gradients are on a logarithmic scale due to the rapid increase as ${x}_i \rightarrow \epsilon $ for $p < 1$.\relax }{figure.caption.31}{}}
\citation{Chartrand07}
\citation{PortniaguineZhdanov02,Ajo-Franklin07,SunLi14}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}IRLS solver}{58}{section.4.3}}
\newlabel{Solve_IRLS}{{4.3}{58}{IRLS solver}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Basic IRLS algorithm}{58}{subsection.4.3.1}}
\@writefile{brf}{\backcite{Chartrand07}{{58}{4.3.1}{subsection.4.3.1}}}
\@writefile{brf}{\backcite{Ajo-Franklin07}{{58}{4.3.1}{subsection.4.3.1}}}
\@writefile{brf}{\backcite{PortniaguineZhdanov02}{{58}{4.3.1}{subsection.4.3.1}}}
\@writefile{brf}{\backcite{SunLi14}{{58}{4.3.1}{subsection.4.3.1}}}
\citation{LastKubik83,Ajo-Franklin07,Stocco09}
\citation{Ajo-Franklin07}
\citation{LiOldenburg1996}
\newlabel{Convergence}{{4.20}{59}{Basic IRLS algorithm}{equation.4.3.20}{}}
\@writefile{brf}{\backcite{Ajo-Franklin07}{{59}{4.3.1}{equation.4.3.20}}}
\@writefile{brf}{\backcite{LastKubik83}{{59}{4.3.1}{equation.4.3.20}}}
\@writefile{brf}{\backcite{Stocco09}{{59}{4.3.1}{equation.4.3.20}}}
\@writefile{brf}{\backcite{Ajo-Franklin07}{{59}{4.3.1}{equation.4.3.20}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces IRLS Solver 1: Fix parameters\relax }}{59}{table.caption.32}}
\newlabel{tbl:IRLS_v1}{{4.1}{59}{IRLS Solver 1: Fix parameters\relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}1-D synthetic example}{60}{subsection.4.3.2}}
\@writefile{brf}{\backcite{LiOldenburg1996}{{60}{4.3.2}{subsection.4.3.2}}}
\newlabel{eq:9}{{4.21}{60}{1-D synthetic example}{equation.4.3.21}{}}
\newlabel{eq:kernel_1D}{{4.22}{60}{1-D synthetic example}{equation.4.3.22}{}}
\newlabel{eq:Weighted_misifit}{{4.23}{60}{1-D synthetic example}{equation.4.3.23}{}}
\newlabel{eq:Lp_phi_int}{{4.24}{60}{1-D synthetic example}{equation.4.3.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces (a) Synthetic 1D model made up of a rectangular pulse and a Gaussian function. (b) Kernel functions consisting of exponentially decaying cosin functions of the form $ f_j(x) = e^{-j\tmspace  +\medmuskip {.2222em}x }\cdot cos(2 \pi j x)$. (c) Data generated from $\mathbf  {d =F \tmspace  +\thickmuskip {.2777em} m}$ , with $5\%$ random Gaussian noise added. \relax }}{61}{figure.caption.33}}
\newlabel{fig:1D_model}{{4.4}{61}{(a) Synthetic 1D model made up of a rectangular pulse and a Gaussian function. (b) Kernel functions consisting of exponentially decaying cosin functions of the form $ f_j(x) = e^{-j\:x }\cdot cos(2 \pi j x)$. (c) Data generated from $\mathbf {d =F \; m}$ , with $5\%$ random Gaussian noise added. \relax }{figure.caption.33}{}}
\newlabel{DepthWeight}{{4.25}{61}{1-D synthetic example}{equation.4.3.25}{}}
\newlabel{eq:Lp_phi_1D}{{4.26}{62}{1-D synthetic example}{equation.4.3.26}{}}
\newlabel{eq:Rs_Rx}{{4.27}{62}{1-D synthetic example}{equation.4.3.27}{}}
\newlabel{eq:dphi_dm_inv}{{4.28}{62}{1-D synthetic example}{equation.4.3.28}{}}
\newlabel{eq:lsqr_IRLS_dphi_dm}{{4.29}{62}{1-D synthetic example}{equation.4.3.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces (a) Recovered models from the smooth $l_2$-norm regularization, and (bottom) measure of model misfit and model norm as a function of iterations. Both the rectangular pulse and Gaussian function are recovered at the right location along the $x$-axis, but the solution is smooth and dispersed over the entire model domain. (b) Solution obtained from the IRLS with spasity constraints on the model gradients ({q=0}), using the $l_2$-norm model (a) to initiate the IRLS steps. The algorithm uses a fixed threshold parameter ($\epsilon =1e-8$) and fixed trade-off parameter $\beta $. The final solution is blocky, as expected from the norm chosen, but fails to achieve the target data misfit. Clearly the influence of the regularization function has overtaken the minimization process.\relax }}{64}{figure.caption.34}}
\newlabel{fig:1D_IRLS_algo1}{{4.5}{64}{(a) Recovered models from the smooth $l_2$-norm regularization, and (bottom) measure of model misfit and model norm as a function of iterations. Both the rectangular pulse and Gaussian function are recovered at the right location along the $x$-axis, but the solution is smooth and dispersed over the entire model domain. (b) Solution obtained from the IRLS with spasity constraints on the model gradients ({q=0}), using the $l_2$-norm model (a) to initiate the IRLS steps. The algorithm uses a fixed threshold parameter ($\epsilon =1e-8$) and fixed trade-off parameter $\beta $. The final solution is blocky, as expected from the norm chosen, but fails to achieve the target data misfit. Clearly the influence of the regularization function has overtaken the minimization process.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Regularization scaling}{65}{subsection.4.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces IRLS Solver 2: $\beta $-search\relax }}{65}{table.caption.35}}
\newlabel{tbl:IRLS_v2}{{4.2}{65}{IRLS Solver 2: $\beta $-search\relax }{table.caption.35}{}}
\citation{LastKubik83,BarbosaSilva94,Ajo-Franklin07,Stocco09,SunLi14}
\newlabel{eq:scaled_Lp_phi}{{4.32}{66}{Regularization scaling}{equation.4.3.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces (Top) Recovered models from two different algorithms used to implement the IRLS method for $q=0$ and a fix threshold parameter ($\epsilon =1e-8$). (Bottom) Measure of model misfit and model norm as a function of iterations. (a) The first algorithm searches for an optimal trade-off parameter $\beta ^{(k)}$ between each IRLS step, requiring a solution to multiple sub-inversions. (b) The second algorithm only adjusts $\beta ^{(k)}$ once after each IRLS step. A new scaling parameter $\gamma ^{(k)}$ is added to smooth the transition between the IRLS updates. The algorithm recovers a similar blocky model but is computationally cheaper, as indicated by the total number of beta iterations and CG solves.\relax }}{67}{figure.caption.36}}
\newlabel{fig:1D_IRLS_algo2}{{4.6}{67}{(Top) Recovered models from two different algorithms used to implement the IRLS method for $q=0$ and a fix threshold parameter ($\epsilon =1e-8$). (Bottom) Measure of model misfit and model norm as a function of iterations. (a) The first algorithm searches for an optimal trade-off parameter $\beta ^{(k)}$ between each IRLS step, requiring a solution to multiple sub-inversions. (b) The second algorithm only adjusts $\beta ^{(k)}$ once after each IRLS step. A new scaling parameter $\gamma ^{(k)}$ is added to smooth the transition between the IRLS updates. The algorithm recovers a similar blocky model but is computationally cheaper, as indicated by the total number of beta iterations and CG solves.\relax }{figure.caption.36}{}}
\citation{LastKubik83}
\citation{Ajo-Franklin07}
\citation{LastKubik83}
\citation{Gorodnitsky97}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces IRLS Solver 3: Scaled regularization\relax }}{68}{table.caption.37}}
\newlabel{tbl:IRLS_v3}{{4.3}{68}{IRLS Solver 3: Scaled regularization\relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Threshold parameter $\epsilon $}{68}{subsection.4.3.4}}
\newlabel{eps_algo}{{4.3.4}{68}{Threshold parameter $\epsilon $}{subsection.4.3.4}{}}
\@writefile{brf}{\backcite{Ajo-Franklin07}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{BarbosaSilva94}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{LastKubik83}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{Stocco09}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{SunLi14}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{LastKubik83}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{Ajo-Franklin07}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{LastKubik83}{{68}{4.3.4}{subsection.4.3.4}}}
\@writefile{brf}{\backcite{Gorodnitsky97}{{68}{4.3.4}{subsection.4.3.4}}}
\citation{Portniaguine1999,PortniaguineZhdanov02}
\citation{Chartrand07}
\newlabel{eq:Lp_Jweighted}{{4.33}{69}{Threshold parameter $\epsilon $}{equation.4.3.33}{}}
\newlabel{eq:m_weighted}{{4.34}{69}{Threshold parameter $\epsilon $}{equation.4.3.34}{}}
\@writefile{brf}{\backcite{PortniaguineZhdanov02}{{69}{4.3.4}{equation.4.3.34}}}
\@writefile{brf}{\backcite{Portniaguine1999}{{69}{4.3.4}{equation.4.3.34}}}
\@writefile{brf}{\backcite{Chartrand07}{{69}{4.3.4}{equation.4.3.34}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces IRLS Solver 4: $\epsilon $-Cooling\relax }}{70}{table.caption.38}}
\newlabel{tbl:IRLS_v4}{{4.4}{70}{IRLS Solver 4: $\epsilon $-Cooling\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces (Top) Recovered models from two different algorithms used to implement the IRLS method for $q=0$ with cooling of the threshold parameter $\epsilon $. (Bottom) Measure of model misfit and model norm as a function of iterations. Both algorithms adjust $\beta $ and scaling parameter $\gamma ^{(k)}$ between each IRLS iteration. (a) In the first case, the threshold parameter $\epsilon $ is monotonically reduced until reaching the convergence criteria. The solution is sparser than the previous algorithm, even though $\epsilon $ is much larger than machine error. (b) In the second case, $\epsilon $ is decreased until reaching the target threshold $\epsilon ^*$. The solution is blocky, penalizing small model gradients.\relax }}{71}{figure.caption.39}}
\newlabel{fig:1D_IRLS_algo3}{{4.7}{71}{(Top) Recovered models from two different algorithms used to implement the IRLS method for $q=0$ with cooling of the threshold parameter $\epsilon $. (Bottom) Measure of model misfit and model norm as a function of iterations. Both algorithms adjust $\beta $ and scaling parameter $\gamma ^{(k)}$ between each IRLS iteration. (a) In the first case, the threshold parameter $\epsilon $ is monotonically reduced until reaching the convergence criteria. The solution is sparser than the previous algorithm, even though $\epsilon $ is much larger than machine error. (b) In the second case, $\epsilon $ is decreased until reaching the target threshold $\epsilon ^*$. The solution is blocky, penalizing small model gradients.\relax }{figure.caption.39}{}}
\citation{ZhdanovTolstaya2004}
\@writefile{brf}{\backcite{ZhdanovTolstaya2004}{{72}{4.3.4}{figure.caption.39}}}
\newlabel{s_ms}{{4.36}{72}{Threshold parameter $\epsilon $}{equation.4.3.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces (a) Distribution of model parameters and (b) model gradients recovered from the smooth $l_2$-norm regularization. Both curves show a sharp corner around which the model functions vary rapidly. Similarly, a measure of (c) model norm $s_{MS}$ and (d) model gradient norm $s_{MGS}$ can be computed over a range of $\epsilon $ values, yielding a similar $L-curve$. The point of maximum curvature can be used to determine the optimal $effective\tmspace  +\thickmuskip {.2777em}zero$ parameter $\epsilon _p$ and $\epsilon _q$. \relax }}{73}{figure.caption.40}}
\newlabel{fig:1D_Model_curve}{{4.8}{73}{(a) Distribution of model parameters and (b) model gradients recovered from the smooth $l_2$-norm regularization. Both curves show a sharp corner around which the model functions vary rapidly. Similarly, a measure of (c) model norm $s_{MS}$ and (d) model gradient norm $s_{MGS}$ can be computed over a range of $\epsilon $ values, yielding a similar $L-curve$. The point of maximum curvature can be used to determine the optimal $effective\;zero$ parameter $\epsilon _p$ and $\epsilon _q$. \relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Scaled-IRLS method (S-IRLS)}{74}{section.4.4}}
\newlabel{S-IRLS}{{4.4}{74}{Scaled-IRLS method (S-IRLS)}{section.4.4}{}}
\newlabel{StepDirection}{{4.37}{74}{Scaled-IRLS method (S-IRLS)}{equation.4.4.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces (a) (Top) Recovered model using a mixed-norm regularization for $p=0$, $q=2$, $\epsilon =1e-3$. (Bottom) Measure of model misfit and model norm as a function of iterations. The inversion converges to a sparse solution without apparent penalty on the gradient, indicative of an imbalance in the regularization. (b) Recovered model and convergence curves after rescaling of the regularization, yielding a model that is both sparse and smooth as expected from the applied mixed-norm .\relax }}{75}{figure.caption.41}}
\newlabel{fig:1D_Eta_test}{{4.9}{75}{(a) (Top) Recovered model using a mixed-norm regularization for $p=0$, $q=2$, $\epsilon =1e-3$. (Bottom) Measure of model misfit and model norm as a function of iterations. The inversion converges to a sparse solution without apparent penalty on the gradient, indicative of an imbalance in the regularization. (b) Recovered model and convergence curves after rescaling of the regularization, yielding a model that is both sparse and smooth as expected from the applied mixed-norm .\relax }{figure.caption.41}{}}
\newlabel{eq:Toy_phi}{{4.39}{76}{Scaled-IRLS method (S-IRLS)}{equation.4.4.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Contour maps for (a) the misfit function $\phi _d$, (b) the model norm $\phi _s$ and (c) the norm of model gradients $\phi _x$. (d) The total objective function $\phi (m)$ has a global minimum located at $\mathbf  {m}=(0.5,1.0)$ for a given small trade-off parameter ($\beta = 1e-3$). The direction of update is shown for two starting models $\mathbf  {m}^{(0)}$ (black and white dot). (e) Section through the objective function along the minimum of $\phi _d$. The global minimum occurs where the partial gradients of $\frac  {\partial \phi _s}{\partial \tmspace  +\thickmuskip {.2777em}m_1}$ and $\frac  {\partial \phi _x}{\partial \tmspace  +\thickmuskip {.2777em}m_1}$ have equal and opposite signs.\relax }}{77}{figure.caption.42}}
\newlabel{fig:IRLS_Phis_Phix}{{4.10}{77}{Contour maps for (a) the misfit function $\phi _d$, (b) the model norm $\phi _s$ and (c) the norm of model gradients $\phi _x$. (d) The total objective function $\phi (m)$ has a global minimum located at $\mathbf {m}=(0.5,1.0)$ for a given small trade-off parameter ($\beta = 1e-3$). The direction of update is shown for two starting models $\mathbf {m}^{(0)}$ (black and white dot). (e) Section through the objective function along the minimum of $\phi _d$. The global minimum occurs where the partial gradients of $\frac {\partial \phi _s}{\partial \;m_1}$ and $\frac {\partial \phi _x}{\partial \;m_1}$ have equal and opposite signs.\relax }{figure.caption.42}{}}
\newlabel{eq:eta}{{4.40}{77}{Scaled-IRLS method (S-IRLS)}{equation.4.4.40}{}}
\newlabel{S-lp_dphidm}{{4.41}{78}{Scaled-IRLS method (S-IRLS)}{equation.4.4.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces (a) Partial gradients of approximated $l_p$-norm penalties for a fix stabilizing parameter $\epsilon =1e-2$. Gradients for $p < 1$ are consistently larger on the interval $[0 < x_i < \sqrt  {1-\epsilon ^2}]$, making it hard to combine multiple norm penalties within the same objective function. (b) Function gradients after applying a scale of $\epsilon ^{(1-p/2)}$, forcing each $l_p$-norm to intersect at $m = \sqrt  {\epsilon }$. \relax }}{78}{figure.caption.43}}
\newlabel{fig:Lp_g_scaled}{{4.11}{78}{(a) Partial gradients of approximated $l_p$-norm penalties for a fix stabilizing parameter $\epsilon =1e-2$. Gradients for $p < 1$ are consistently larger on the interval $[0 < x_i < \sqrt {1-\epsilon ^2}]$, making it hard to combine multiple norm penalties within the same objective function. (b) Function gradients after applying a scale of $\epsilon ^{(1-p/2)}$, forcing each $l_p$-norm to intersect at $m = \sqrt {\epsilon }$. \relax }{figure.caption.43}{}}
\newlabel{approx_lp}{{4.42}{78}{Scaled-IRLS method (S-IRLS)}{equation.4.4.42}{}}
\citation{LiOldenburg1996}
\citation{LiOldenburg1996}
\newlabel{eq:S-IRLS}{{4.43}{79}{Scaled-IRLS method (S-IRLS)}{equation.4.4.43}{}}
\newlabel{eq:eta}{{4.44}{79}{Scaled-IRLS method (S-IRLS)}{equation.4.4.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Cell-based weights (${w}_r,\tmspace  +\thickmuskip {.2777em} w_m$)}{79}{subsection.4.4.1}}
\newlabel{Wr_Section}{{4.4.1}{79}{Cell-based weights (${w}_r,\; w_m$)}{subsection.4.4.1}{}}
\@writefile{brf}{\backcite{LiOldenburg1996}{{79}{4.4.1}{subsection.4.4.1}}}
\@writefile{brf}{\backcite{LiOldenburg1996}{{79}{4.4.1}{subsection.4.4.1}}}
\citation{PortniaguineZhdanov02}
\newlabel{eq:phi_Wrm}{{4.45}{80}{Cell-based weights (${w}_r,\; w_m$)}{equation.4.4.45}{}}
\@writefile{brf}{\backcite{PortniaguineZhdanov02}{{80}{4.4.1}{equation.4.4.45}}}
\newlabel{Weighted_IRLS}{{4.46}{80}{Cell-based weights (${w}_r,\; w_m$)}{equation.4.4.46}{}}
\newlabel{Weighted_IRLS_Facto}{{4.47}{81}{Cell-based weights (${w}_r,\; w_m$)}{equation.4.4.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Recovered models for two different depth weighting formulations: (red) weighted sensitivity $\phi (\mathaccentV {hat}05Em)$, (black) weighted regularization $\phi (m)$. (a) True and recovered models using the $\phi (\mathaccentV {hat}05Em)$ and $\phi (m)$ formulations for penalty applied on the model gradients for $q=0$ and (b) for $p= 0,\tmspace  +\thickmuskip {.2777em}q=2$. The weighted sensitivity formulation $\phi (\mathaccentV {hat}05Em)$ increases the influence the regularization function with distance along the $x$-axis, skewing the model towards the right.\relax }}{81}{figure.caption.44}}
\newlabel{fig:1D_Wr_in_vs_out}{{4.12}{81}{Recovered models for two different depth weighting formulations: (red) weighted sensitivity $\phi (\hat m)$, (black) weighted regularization $\phi (m)$. (a) True and recovered models using the $\phi (\hat m)$ and $\phi (m)$ formulations for penalty applied on the model gradients for $q=0$ and (b) for $p= 0,\;q=2$. The weighted sensitivity formulation $\phi (\hat m)$ increases the influence the regularization function with distance along the $x$-axis, skewing the model towards the right.\relax }{figure.caption.44}{}}
\citation{SunLi14}
\citation{SunLi14}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Mixed $l_p$-norm regularization}{82}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Localized S-IRLS}{82}{subsection.4.5.1}}
\newlabel{Localized_lp}{{4.5.1}{82}{Localized S-IRLS}{subsection.4.5.1}{}}
\@writefile{brf}{\backcite{SunLi14}{{82}{4.5.1}{subsection.4.5.1}}}
\@writefile{brf}{\backcite{SunLi14}{{82}{4.5.1}{subsection.4.5.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces (a) Model error $\delimiter "026B30D  m - m{*}\delimiter "026B30D _1$ and (b) misfit function for the 441 inverted models using a range of regularization with mixed-norm penalty on the model for $0 \leq p \leq 2$ and on model gradients for $0 \leq q \leq 2$. (c) The largest model error ($\delimiter "026B30D  \delta \mathbf  {m}\delimiter "026B30D _1$) was obtained with the mixed-norm for $p=0,\tmspace  +\thickmuskip {.2777em}q=2$, compared to (d) the optimal solution found with $p=1.5$ and $q=0.4$.\relax }}{83}{figure.caption.45}}
\newlabel{fig:1D_Results_ALL}{{4.13}{83}{(a) Model error $\| m - m{*}\|_1$ and (b) misfit function for the 441 inverted models using a range of regularization with mixed-norm penalty on the model for $0 \leq p \leq 2$ and on model gradients for $0 \leq q \leq 2$. (c) The largest model error ($\| \delta \mathbf {m}\|_1$) was obtained with the mixed-norm for $p=0,\;q=2$, compared to (d) the optimal solution found with $p=1.5$ and $q=0.4$.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces (a) Nine of the 441 inverted models for a range of mixed-norm penalties on the model and its gradient for $0 \leq p \leq 2$ and $0 \leq q \leq 2$. \relax }}{84}{figure.caption.46}}
\newlabel{fig:1D_Results}{{4.14}{84}{(a) Nine of the 441 inverted models for a range of mixed-norm penalties on the model and its gradient for $0 \leq p \leq 2$ and $0 \leq q \leq 2$. \relax }{figure.caption.46}{}}
\newlabel{eq:Subsets}{{4.5.1}{84}{Localized S-IRLS}{subsection.4.5.1}{}}
\newlabel{p_vector}{{4.48}{85}{Localized S-IRLS}{equation.4.5.48}{}}
\newlabel{smooth_p_vector}{{4.49}{85}{Localized S-IRLS}{equation.4.5.49}{}}
\newlabel{eq:Phi_IRLS}{{4.50}{85}{Localized S-IRLS}{equation.4.5.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces (Left) Improved solution for the 1-D problem after applying a localized mixed-norm penalty, where the regularization is divided into two regions with independent $l_p$-norm regularization: (left) $p = q = 0$, (right) $p=1 , q=2$. (Right) Convergence curves for the mixed-norm S-IRLS inversion.\relax }}{86}{figure.caption.47}}
\newlabel{fig:1D_Mixed_norm}{{4.15}{86}{(Left) Improved solution for the 1-D problem after applying a localized mixed-norm penalty, where the regularization is divided into two regions with independent $l_p$-norm regularization: (left) $p = q = 0$, (right) $p=1 , q=2$. (Right) Convergence curves for the mixed-norm S-IRLS inversion.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}2-D example}{87}{subsection.4.5.2}}
\newlabel{2D_Section}{{4.5.2}{87}{2-D example}{subsection.4.5.2}{}}
\newlabel{eq:grad}{{4.53}{87}{2-D example}{equation.4.5.53}{}}
\newlabel{eq:mag_grad}{{4.54}{87}{2-D example}{equation.4.5.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces (a) Synthetic 2-D model made up of a square block and a smooth Gaussian function. (b) Example of a kernel function for $e^{-\omega \tmspace  +\medmuskip {.2222em}r}\cdot cos(2 \pi \omega r)$ and (c) data generated from $\mathbf  {d =F \tmspace  +\thickmuskip {.2777em} m}$. Five percent random Gaussian noise is added. \relax }}{88}{figure.caption.48}}
\newlabel{fig:2D_Model_Kernel_data}{{4.16}{88}{(a) Synthetic 2-D model made up of a square block and a smooth Gaussian function. (b) Example of a kernel function for $e^{-\omega \:r}\cdot cos(2 \pi \omega r)$ and (c) data generated from $\mathbf {d =F \; m}$. Five percent random Gaussian noise is added. \relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces (a) Recovered model for $p = q_x = q_z = 1$ penalizing finite difference gradients in orthogonal directions, yielding right-angled anomalies. (b) Recovered model for the same norms but penalizing the absolute gradient of the model ($|\nabla \mathbf  {m}|$) recovering round edges.\relax }}{90}{figure.caption.49}}
\newlabel{fig:2D_Model_Grad_Test}{{4.17}{90}{(a) Recovered model for $p = q_x = q_z = 1$ penalizing finite difference gradients in orthogonal directions, yielding right-angled anomalies. (b) Recovered model for the same norms but penalizing the absolute gradient of the model ($|\nabla \mathbf {m}|$) recovering round edges.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Distribution of $l_p$-norm on the model and model gradients over the 2-D model domain. The original boundary of each region was smoothed in order to get a slow transition and reduce visible artifacts. Regions were chosen to cover a larger area than the anomalies to simulate a blind-inversion. \relax }}{90}{figure.caption.50}}
\newlabel{fig:2D_Mix_lp_norms}{{4.18}{90}{Distribution of $l_p$-norm on the model and model gradients over the 2-D model domain. The original boundary of each region was smoothed in order to get a slow transition and reduce visible artifacts. Regions were chosen to cover a larger area than the anomalies to simulate a blind-inversion. \relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces (a) Smooth $l_2$-norm solution used to initiate the IRLS iterations. (b) Recovered model using the mixed-norm regularization after seven S-IRLS iterations. The contour line (red) marks the value of $\epsilon _p$, judged to be the \emph  {effective zero} value of the model ($m_i \leq $ 5e-2). Both models (a) and (b) fit the data within 2\% of the target misfit $\phi _d^*$. (c) Dual plots showing the distribution of model parameters and the gradient of the $l_0$-norm penalty function $\mathbf  {\mathaccentV {hat}05Eg_p}(m)$ as a function of S-IRLS iterations. High penalties are applied to progressively smaller model values. The final model nicely recovers both the blocky and smooth Gaussian anomaly.\relax }}{91}{figure.caption.51}}
\newlabel{fig:2D_Mix_norm_result}{{4.19}{91}{(a) Smooth $l_2$-norm solution used to initiate the IRLS iterations. (b) Recovered model using the mixed-norm regularization after seven S-IRLS iterations. The contour line (red) marks the value of $\epsilon _p$, judged to be the \emph {effective zero} value of the model ($m_i \leq $ 5e-2). Both models (a) and (b) fit the data within 2\% of the target misfit $\phi _d^*$. (c) Dual plots showing the distribution of model parameters and the gradient of the $l_0$-norm penalty function $\mathbf {\hat g_p}(m)$ as a function of S-IRLS iterations. High penalties are applied to progressively smaller model values. The final model nicely recovers both the blocky and smooth Gaussian anomaly.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}3-D example}{92}{subsection.4.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces  (a) Iso-surface (0.002 SI) and (b) sections through the recovered susceptibility model after five IRLS iterations for $(p = 0,\tmspace  +\thickmuskip {.2777em} q = 2)$. The final model is substantially closer to the true solution.\relax }}{93}{figure.caption.52}}
\newlabel{fig:3D_Inv_l0l2_model_INDUCED}{{4.20}{93}{(a) Iso-surface (0.002 SI) and (b) sections through the recovered susceptibility model after five IRLS iterations for $(p = 0,\; q = 2)$. The final model is substantially closer to the true solution.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces  Comparison between (a) observed and (b) predicted data from the recovered susceptibility model using compact norms for $(p = 0,\tmspace  +\thickmuskip {.2777em} q = 2)$. (c) Normalized data residuals are within two standard deviations.\relax }}{93}{figure.caption.53}}
\newlabel{fig:3D_Inv_l0l2_pred_INDUCED}{{4.21}{93}{Comparison between (a) observed and (b) predicted data from the recovered susceptibility model using compact norms for $(p = 0,\; q = 2)$. (c) Normalized data residuals are within two standard deviations.\relax }{figure.caption.53}{}}
\citation{DoyleEtAl1999}
\citation{Wilkinson2001}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces  (a) Iso-surface (0.002 SI) and (b) sections through the recovered susceptibility model after nine IRLS iterations for $(p = 0,\tmspace  +\thickmuskip {.2777em} q = 1)$ . Sparsity constraints on the model and model gradients yield a simple and blocky model.\relax }}{94}{figure.caption.54}}
\newlabel{fig:3D_Inv_l0l0_model_INDUCED}{{4.22}{94}{(a) Iso-surface (0.002 SI) and (b) sections through the recovered susceptibility model after nine IRLS iterations for $(p = 0,\; q = 1)$ . Sparsity constraints on the model and model gradients yield a simple and blocky model.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces  Comparison between (a) observed and (b) predicted data from the recovered susceptibility model using compact norms for $(p = 0,\tmspace  +\thickmuskip {.2777em} q = 1)$. (c) Normalized data residuals are within two standard deviations.\relax }}{94}{figure.caption.55}}
\newlabel{fig:3D_Inv_l0l0_pred_INDUCED}{{4.23}{94}{Comparison between (a) observed and (b) predicted data from the recovered susceptibility model using compact norms for $(p = 0,\; q = 1)$. (c) Normalized data residuals are within two standard deviations.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Case study - Tli Kwi Cho kimberlite complex}{95}{section.4.6}}
\newlabel{3D_Section}{{4.6}{95}{Case study - Tli Kwi Cho kimberlite complex}{section.4.6}{}}
\@writefile{brf}{\backcite{DoyleEtAl1999}{{95}{4.6}{section.4.6}}}
\@writefile{brf}{\backcite{Wilkinson2001}{{95}{4.6}{section.4.6}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces (a) (Left) Horizontal section through the recovered susceptibility model at 25 m depth below topography from the smooth $l_2$-norm regularization. (Right) Iso-surface of susceptibility values around 0.002 SI. (b) Recovered model using the mixed-norm S-IRLS algorithm. Magnetic dykes are better recovered, imaged as continuous plates and extending vertically at depth. Susceptibility values for DO-27 and DO-18 have increased, showing as compact vertical pipes.\relax }}{97}{figure.caption.56}}
\newlabel{fig:TKC_Mag_Models}{{4.24}{97}{(a) (Left) Horizontal section through the recovered susceptibility model at 25 m depth below topography from the smooth $l_2$-norm regularization. (Right) Iso-surface of susceptibility values around 0.002 SI. (b) Recovered model using the mixed-norm S-IRLS algorithm. Magnetic dykes are better recovered, imaged as continuous plates and extending vertically at depth. Susceptibility values for DO-27 and DO-18 have increased, showing as compact vertical pipes.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary}{98}{section.4.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Horizontal section through the mixed-norm models applied to four sub-regions with smooth transition across zones.\relax }}{99}{figure.caption.57}}
\newlabel{fig:TKC_Mix_Norm}{{4.25}{99}{Horizontal section through the mixed-norm models applied to four sub-regions with smooth transition across zones.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces (a) Observed and predicted data over the TKC kimberlite complex. (b) Residuals between observed and predicted data normalized by the estimated uncertainties (10 nT). Both the smooth and mixed-norm inversions reproduce the data within four standard deviations.\relax }}{100}{figure.caption.58}}
\newlabel{fig:TKC_Obs_vs_Pred}{{4.26}{100}{(a) Observed and predicted data over the TKC kimberlite complex. (b) Residuals between observed and predicted data normalized by the estimated uncertainties (10 nT). Both the smooth and mixed-norm inversions reproduce the data within four standard deviations.\relax }{figure.caption.58}{}}
\@setckpt{Chap4_Mixed_Lpnorm_Regularization}{
\setcounter{page}{101}
\setcounter{equation}{55}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{7}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{26}
\setcounter{table}{4}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{NAT@ctr}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{39}
\setcounter{lstlisting}{0}
\setcounter{section@level}{0}
}
